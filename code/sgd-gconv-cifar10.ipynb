{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sgd-gconv-cifar10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IJWC1hilYW8e"},"source":["import tensorflow.keras as tfk\n","from tensorflow.keras.losses import *\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.utils import *\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tqdm import *\n","import pickle\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ufxj30yVYW8q"},"source":["Download cifar10"]},{"cell_type":"code","metadata":{"id":"oE9-mqC3YW8s"},"source":["(x_train, y_train), (x_test, y_test) = tfk.datasets.cifar10.load_data()\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2KH9fdMSYW8z"},"source":["y_train = to_categorical(y_train, num_classes=10)\n","y_test = to_categorical(y_test, num_classes=10)\n","x_train = x_train / 255.0\n","x_test = x_test / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2_sD2lDWYW86"},"source":["Model"]},{"cell_type":"code","metadata":{"id":"sI4EdBd2YW88"},"source":["class SignGate(Layer):\n","    def __init__(self, **kwargs):\n","        super(SignGate, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        super(SignGate, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, x):\n","        output = K.sign(K.relu(x))\n","\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","def getNetwork1Layers(model):\n","    pattern = re.compile(\"^n1_*\")\n","    layer_list  = []\n","\n","    for layer in model.layers:\n","        if pattern.match(layer.name):\n","            layer_list.append(layer)\n","\n","    return layer_list\n","\n","def getNetwork2Layers(model):\n","    pattern = re.compile(\"^n2_*\")\n","    layer_list  = []\n","\n","    for layer in model.layers:\n","        if pattern.match(layer.name):\n","            layer_list.append(layer)\n","\n","    return layer_list\n","\n","def freezeWeights(layers):\n","    for layer in layers:\n","        layer.trainable = False\n","\n","img_wid = 32\n","def getConv4Relu():\n","    inputs = Input(shape = (img_wid, img_wid, 3))\n","    \n","    C1 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c1\",\n","                           activation = 'relu')(inputs)\n","    C2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c2\",\n","                           activation = 'relu')(C1)\n","    C3 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c3\",\n","                           activation = 'relu')(C2)\n","    C4 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c4\",\n","                           activation = 'relu')(C3)\n","\n","    G1 = GlobalAveragePooling2D()(C4)\n","    F1 = Flatten()(G1)\n","    D1 = Dense(units = 256, activation = 'relu', name = \"n1_d1\")(F1)\n","    D2 = Dense(units = 256, activation = 'relu', name = \"n1_d2\")(D1)\n","\n","    #Output\n","    outputs = Dense(units = 10, activation = 'softmax', name = \"output\")(D2)\n","\n","    model = tfk.Model(inputs = inputs, outputs = outputs)\n","\n","    return model\n","\n","def getConv4Galu():\n","    inputs = Input(shape = (img_wid, img_wid, 3))\n","    #V1\n","    C1 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c1\",\n","                           activation = 'relu')(inputs)\n","    C2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c2\",\n","                           activation = 'relu')(C1)\n","    C3 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c3\",\n","                           activation = 'relu')(C2)\n","    C4 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c4\",\n","                           activation = 'relu')(C3)\n","    G1 = GlobalAveragePooling2D()(C4)\n","    F1 = Flatten()(G1)\n","    D1 = Dense(units = 256, activation = 'relu', name = \"n1_d1\")(F1)\n","    D2 = Dense(units = 256, activation = 'relu', name = \"n1_d2\")(D1)\n","\n","    A1 = SignGate()(C1)\n","    A2 = SignGate()(C2)\n","    A3 = SignGate()(C3)\n","    A4 = SignGate()(C4)\n","    A5 = SignGate()(D1)\n","    A6 = SignGate()(D2)\n","    C1_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c1\",\n","                           activation = 'linear')(inputs)\n","    C1_G = Multiply()([A1, C1_G])\n","\n","    C2_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c2\", \n","                           activation = 'linear')(C1_G)\n","    C2_G = Multiply()([A2, C2_G])\n","\n","    #V2\n","    C3_G = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n2_c3\",\n","                           activation = 'linear')(C2_G)\n","    C3_G = Multiply()([A3, C3_G])\n","\n","    C4_G = Conv2D(filters = 128, kernel_size = (3, 3),  padding = 'same', name = \"n2_c4\",\n","                           activation = 'linear')(C3_G)\n","    C4_G = Multiply()([A4, C4_G])\n","\n","    G1_G = GlobalAveragePooling2D()(C4_G)\n","    F1_G = Flatten()(G1_G)\n","\n","    D1_G = Dense(units = 256, activation = 'linear', name = \"n2_d1\")(F1_G)\n","    D1_G = Multiply()([A5, D1_G])\n","\n","    D2_G = Dense(units = 256, activation = 'linear', name = \"n2_d2\")(D1_G)\n","    D2_G = Multiply()([A6, D2_G])\n","\n","    outputs = Dense(units = 10, activation = 'softmax', name = \"output\")(D2_G)\n","\n","    model = tfk.Model(inputs = inputs, outputs = outputs)\n","    return model\n","\n","eps, beta = 0.1, 4\n","\n","class SoftGate(Layer):\n","    def __init__(self, **kwargs):\n","        super(SoftGate, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        super(SoftGate, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, x):\n","        activation = (1 + eps)*K.sigmoid(beta*x)\n","        return activation\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","def getDecoupledLearning():\n","    inputs = Input(shape = (img_wid, img_wid, 3))\n","    #V1\n","    C1 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c1\",\n","                           activation = 'linear')(inputs)\n","    C1_A = Activation('relu')(C1)\n","    C2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c2\",\n","                           activation = 'linear')(C1_A)\n","    C2_A = Activation('relu')(C2)\n","    C3 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c3\",\n","                           activation = 'linear')(C2_A)\n","    C3_A = Activation('relu')(C3)\n","    C4 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c4\",\n","                           activation = 'linear')(C3_A)\n","    C4_A = Activation('relu')(C4)\n","    G1 = GlobalAveragePooling2D()(C4_A)\n","    F1 = Flatten()(G1)\n","    D1 = Dense(units = 256, activation = 'linear', name = \"n1_d1\")(F1)\n","    D1_A = Activation('relu')(D1)\n","    D2 = Dense(units = 256, activation = 'linear', name = \"n1_d2\")(D1_A)\n","\n","    A1 = SoftGate()(C1)\n","    A2 = SoftGate()(C2)\n","    A3 = SoftGate()(C3)\n","    A4 = SoftGate()(C4)\n","    A5 = SoftGate()(D1)\n","    A6 = SoftGate()(D2)\n","    C1_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c1\",\n","                           activation = 'linear')(inputs)\n","    C1_G = Multiply()([A1, C1_G])\n","\n","    C2_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c2\", \n","                           activation = 'linear')(C1_G)\n","    C2_G = Multiply()([A2, C2_G])\n","\n","    #V2\n","    C3_G = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n2_c3\",\n","                           activation = 'linear')(C2_G)\n","    C3_G = Multiply()([A3, C3_G])\n","\n","    C4_G = Conv2D(filters = 128, kernel_size = (3, 3),  padding = 'same', name = \"n2_c4\",\n","                           activation = 'linear')(C3_G)\n","    C4_G = Multiply()([A4, C4_G])\n","\n","    G1_G = GlobalAveragePooling2D()(C4_G)\n","    F1_G = Flatten()(G1_G)\n","\n","    D1_G = Dense(units = 256, activation = 'linear', name = \"n2_d1\")(F1_G)\n","    D1_G = Multiply()([A5, D1_G])\n","\n","    D2_G = Dense(units = 256, activation = 'linear', name = \"n2_d2\")(D1_G)\n","    D2_G = Multiply()([A6, D2_G])\n","\n","    outputs = Dense(units = 10, activation = 'softmax', name = \"output\")(D2_G)\n","\n","    model = tfk.Model(inputs = inputs, outputs = outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyMUa1sFYW9D"},"source":["loss = tfk.losses.categorical_crossentropy\n","opt = tfk.optimizers.SGD\n","batch_size = 32\n","num_exp = 5\n","num_epochs = 100\n","\n","history_relu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n","history_flnpf = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n","history_frnpf_di = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n","history_frnpf_ii = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n","history_dlnpf = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1C3PD_XeYW9K"},"source":["Train: Standard ReLU and FLNPF"]},{"cell_type":"code","metadata":{"id":"CSXAyQjsYW9L"},"source":["lr=1e-1   \n","for exp_i in range(num_exp):\n","    print(\"_____________EXP:{}____________\".format(exp_i+1))\n","\n","    model_relu = getConv4Relu()\n","    model_relu.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n","\n","    filepath=\"weights.best.hdf5\"\n","    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n","    callbacks_list = [checkpoint]\n","\n","    history = model_relu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n","                                batch_size=batch_size, epochs= num_epochs, callbacks = callbacks_list)\n","    history_relu['acc'].append(history.history['acc'])\n","    history_relu['val_acc'].append(history.history['val_acc'])\n","    history_relu['loss'].append(history.history['loss'])\n","    history_relu['val_loss'].append(history.history['val_loss'])\n","    print(\"ReLU: MAX ACC = {}, MAX VAL ACC = {}\".format(np.max(history.history['acc']), \n","                                                        np.max(history.history['val_acc'])))\n","    model_relu.load_weights(\"weights.best.hdf5\")\n","\n","    model_flnpf = getConv4Galu()\n","    layers_relu = getNetwork1Layers(model_relu)\n","    layers_flnpf = getNetwork1Layers(model_flnpf)\n","\n","    for layer in layers_flnpf:\n","        layer.trainable = False\n","\n","    model_flnpf.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n","\n","    for layer1,layer2 in zip(layers_flnpf, layers_relu): \n","        layer1.set_weights(layer2.get_weights())\n","\n","    history = model_flnpf.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n","                                     batch_size=batch_size, epochs= num_epochs)\n","    history_flnpf['acc'].append(history.history['acc'])\n","    history_flnpf['val_acc'].append(history.history['val_acc'])\n","    history_flnpf['loss'].append(history.history['loss'])\n","    history_flnpf['val_loss'].append(history.history['val_loss'])\n","\n","    print(\"FLNPF: MAX ACC = {}, MAX VAL ACC = {}\".format(np.max(history.history['acc']), \n","                                                        np.max(history.history['val_acc'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLOcwO60YW9S"},"source":["print(\"ReLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n","                                                    np.mean(np.max(history_relu['acc'], axis = 1)), \n","                                                    np.mean(np.max(history_relu['val_acc'], axis = 1)),\n","                                                    np.std(np.max(history_relu['val_acc'], axis = 1))))\n","\n","print(\"FLNPF: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n","                                                    np.mean(np.max(history_flnpf['acc'], axis = 1)), \n","                                                    np.mean(np.max(history_flnpf['val_acc'], axis = 1)),\n","                                                    np.std(np.max(history_flnpf['val_acc'], axis = 1))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L33Uwb-dYW9b"},"source":["file = open(\"h_cifar10_gconv_sgd_relu\", 'wb')\n","pickle.dump(history_relu, file)\n","\n","file = open(\"h_cifar10_gconv_sgd_flnpf\", 'wb')\n","pickle.dump(history_flnpf, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dR8ufBohYW9i"},"source":["- FRNPF(DI)"]},{"cell_type":"code","metadata":{"id":"MLEIVQfiYW9k"},"source":["lr = 1e-1\n","for exp_i in range(num_exp):\n","    print(\"_____________EXP:{}____________\".format(exp_i+1))\n","    model_frnpf_di = getConv4Galu()\n","    layers_frnpf_di_n1 = getNetwork1Layers(model_frnpf_di)\n","    layers_frnpf_di_n2 = getNetwork2Layers(model_frnpf_di)\n","\n","    for layer in layers_frnpf_di_n1:\n","        layer.trainable = False\n","\n","    model_frnpf_di.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n","\n","    for layer1, layer2 in zip(layers_frnpf_di_n1, layers_frnpf_di_n2):\n","        layer1.set_weights(layer2.get_weights())    \n","\n","    history = model_frnpf_di.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n","                                batch_size=batch_size, epochs= num_epochs)\n","    history_frnpf_di['acc'].append(history.history['acc'])\n","    history_frnpf_di['val_acc'].append(history.history['val_acc'])\n","    history_frnpf_di['loss'].append(history.history['loss'])\n","    history_frnpf_di['val_loss'].append(history.history['val_loss'])\n","\n","    print(\"FRNPF(DI): MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(np.max(history.history['acc']), \n","                                                        np.max(history.history['val_acc'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBt0HWxPYW9r"},"source":["print(\"FRNPF(DI): max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n","                                                    np.mean(np.max(history_frnpf_di['acc'], axis = 1)), \n","                                                    np.mean(np.max(history_frnpf_di['val_acc'], axis = 1)),\n","                                                    np.std(np.max(history_frnpf_di['val_acc'], axis = 1))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KuvJHAZYW9x"},"source":["file = open(\"h_cifar10_gconv_sgd_frnpf_di\", 'wb')\n","pickle.dump(history_frnpf_di, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDoHwxVmYW94"},"source":["- FRNPF(II)"]},{"cell_type":"code","metadata":{"id":"SvQTqAElYW95"},"source":["lr = 1e-1\n","for exp_i in range(num_exp):\n","    print(\"_____________EXP:{}____________\".format(exp_i+1))\n","    model_frnpf_ii = getConv4Galu()\n","    layers_frnpf_ii_n1 = getNetwork1Layers(model_frnpf_ii)\n","\n","    for layer in layers_frnpf_ii_n1:\n","        layer.trainable = False\n","\n","    model_frnpf_ii.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n","    history = model_frnpf_ii.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n","                                batch_size=batch_size, epochs= num_epochs)    \n","    history_frnpf_ii['acc'].append(history.history['acc'])\n","    history_frnpf_ii['val_acc'].append(history.history['val_acc'])\n","    history_frnpf_ii['loss'].append(history.history['loss'])\n","    history_frnpf_ii['val_loss'].append(history.history['val_loss'])\n","\n","    print(\"FRNPF(II): MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(np.max(history.history['acc']), \n","                                                        np.max(history.history['val_acc'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGEwjpHhYW-A"},"source":["print(\"FRNPF(II): max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n","                                                    np.mean(np.max(history_frnpf_ii['acc'], axis = 1)), \n","                                                    np.mean(np.max(history_frnpf_ii['val_acc'], axis = 1)),\n","                                                    np.std(np.max(history_frnpf_ii['val_acc'], axis = 1))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1dzdxmHYW-J"},"source":["file = open(\"h_cifar10_gconv_sgd_gap_frnpf_ii\", 'wb')\n","pickle.dump(history_frnpf_ii, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bC7s-IrqYW-R"},"source":["- DLNPF"]},{"cell_type":"code","metadata":{"id":"HCCHsiYlYW-T"},"source":["lr = 1e-1\n","for exp_i in range(num_exp):\n","    print(\"_____________EXP:{}____________\".format(exp_i+1))\n","    model_dlnpf = getDecoupledLearning()\n","    model_dlnpf.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n","\n","    history = model_dlnpf.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n","                             batch_size=batch_size, epochs= num_epochs)\n","    \n","    history_dlnpf['acc'].append(history.history['acc'])\n","    history_dlnpf['val_acc'].append(history.history['val_acc'])\n","    history_dlnpf['loss'].append(history.history['loss'])\n","    history_dlnpf['val_loss'].append(history.history['val_loss'])\n","\n","    print(\"dlnpf: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(\n","                                                        np.max(history.history['acc']), \n","                                                        np.max(history.history['val_acc'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdFvkoeCYW-b"},"source":["print(\"dlnpf: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n","                                            np.mean(np.max(history_dlnpf['acc'], axis = 1)), \n","                                            np.mean(np.max(history_dlnpf['val_acc'], axis = 1)),\n","                                            np.std(np.max(history_dlnpf['val_acc'], axis = 1))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9IkJ5DjYW-j"},"source":["file = open('h_cifar10_gconv_sgd_dlnpf', 'wb')\n","pickle.dump(history_dlnpf, file)"],"execution_count":null,"outputs":[]}]}